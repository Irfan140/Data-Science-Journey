{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # For reading data from different data sources like excel, csv etc\n",
    "import matplotlib.pyplot as plt # For plottling the data\n",
    "import numpy as np # It which provides support for large, multi-dimensional arrays and matrices, along with mathematical functions.\n",
    "%matplotlib inline\n",
    "# A magic command for Jupyter Notebooks. It ensures that all the plots generated using matplotlib are displayed inline (within the notebook).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('height-weight.csv')\n",
    "'''\n",
    "\n",
    "This function reads a CSV (Comma-Separated Values) file and loads its contents into a Pandas DataFrame.\n",
    "The variable df will store the loaded data as a DataFrame.\n",
    "A DataFrame is a two-dimensional, tabular data structure in Pandas, similar to a table in a database or a spreadsheet.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "# it displays the first five rows of the DataFrame by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##scatter plot\n",
    "# This creates a scatter plot with Weight on the x-axis and Height on the y-axis.\n",
    "# Each point represents a row from the DataFrame (df), showing the relationship between Weight and Height.\n",
    "plt.scatter(df['Weight'],df['Height'])\n",
    "\n",
    "\n",
    "plt.xlabel(\"Weight\") # Labels the x-axis as \"Weight\".\n",
    "plt.ylabel(\"Height\") # Labels the y-axis as \"Height\".\n",
    "\n",
    "\n",
    "#  From the graph we can see that when the weight increases the height also increases,so there is a positive correlation between height and weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding Correlation\n",
    "df.corr()\n",
    "# df.corr(method='pearson')\n",
    "# method: Specifies the correlation method. Options are:\n",
    "# 'pearson' (default): Measures linear correlation.\n",
    "# 'kendall': Measures rank correlation.\n",
    "# 'spearman': Measures rank correlation (based on ordinal rankings).\n",
    "\n",
    "\n",
    "# The output is a square matrix (as a DataFrame) where:\n",
    "# Rows and columns represent the numerical columns in df.\n",
    "# Each cell contains the correlation coefficient between two variables.\n",
    "\n",
    "\n",
    "#  OUTPUT\n",
    "# Diagonal Values (1.0):\n",
    "\n",
    "# Weight correlates perfectly with itself (1.0), and the same goes for Height.\n",
    "# This is expected because the correlation of a variable with itself is always 1.0.\n",
    "# Off-Diagonal Value (0.931142):\n",
    "\n",
    "# The correlation coefficient between Weight and Height is approximately 0.93.\n",
    "# Significance:\n",
    "# This is a strong positive correlation. As Weight increases, Height tends to increase as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seaborn for visualization\n",
    "import seaborn as sns\n",
    "sns.pairplot(df)\n",
    "\n",
    "# This function creates a grid of scatter plots for all numerical columns in the DataFrame (df).\n",
    "# It shows pairwise relationships between variables and includes histograms or KDE plots on the diagonal by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Independent and dependent features\n",
    "\n",
    "\n",
    "### independent features should be data frame or 2 dimesnionalarray\n",
    "# X: Independent features (inputs or predictors) that influence the dependent variable.\n",
    "# The double brackets ([['Weight']]) ensure X is a DataFrame (2-dimensional array), as most machine learning libraries like Scikit-Learn require this format for independent features.\n",
    "X=df[['Weight']] \n",
    "\n",
    "\n",
    "## this variable can be in series or 1d array\n",
    "# y: Dependent feature (target or output) that the model will predict.\n",
    "# This can be a Pandas Series (1-dimensional array) or NumPy array.\n",
    "y=df['Height'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_series=df['Weight'] # Extracts the Weight column as a Pandas Series (1D structure).\n",
    "np.array(X_series).shape # Converts the Pandas Series into a NumPy array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y).shape # Converts the Pandas Series into a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "'''\n",
    "Purpose:\n",
    "train_test_split is used to split the dataset into two parts:\n",
    "\n",
    "Training set: Used to train the machine learning model.\n",
    "Test set: Used to evaluate the model's performance on unseen data.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=42)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "1. train_test_split:\n",
    "\n",
    "Splits the dataset (X for features and y for target) into training and testing sets.\n",
    "\n",
    "\n",
    "2. test_size=0.25:\n",
    "\n",
    "This parameter defines the proportion of the dataset to include in the test split.\n",
    "In this case, 25% of the data will be used for testing, and the remaining 75% will be used for training.\n",
    "\n",
    "\n",
    "\n",
    "3. random_state=42:\n",
    "\n",
    "Ensures reproducibility of the data split. By using the same random_state value (e.g., 42), you ensure the dataset is split in the same way each time you run the code.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# StandardScaler from sklearn.preprocessing is used to standardize the features (independent variables).\n",
    "\n",
    "'''\n",
    "\n",
    "1. What is Standardization?\n",
    "\n",
    "Standardization (also known as Z-score normalization) is the process of scaling the features so that they have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Itâ€™s important when the features have different units or magnitudes (e.g., height in centimeters and weight in kilograms).\n",
    "\n",
    "Standardization helps machine learning algorithms, especially those relying on distance metrics (e.g., linear regression, k-nearest neighbors, and gradient descent), to work efficiently.\n",
    "\n",
    "\n",
    "2. When to Use Standardization:\n",
    "\n",
    "Use it when your features are measured in different scales (e.g., height in meters and weight in kilograms) or when algorithms rely on distance-based calculations.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "\n",
    "'''\n",
    "\n",
    "1. scaler = StandardScaler():\n",
    "\n",
    "Initializes the StandardScaler object, which will be used to standardize the data.\n",
    "\n",
    "\n",
    "2. X_train = scaler.fit_transform(X_train):\n",
    "\n",
    "fit: This calculates the mean and standard deviation of the X_train dataset.\n",
    "\n",
    "transform: This scales the data by subtracting the mean and dividing by the standard deviation. This results in a dataset where the mean is 0 and the standard deviation is 1.\n",
    "\n",
    "Note: The transformation is applied in-place, meaning X_train is updated with the standardized values.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=scaler.transform(X_test)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "1. transform is applied to the X_test data using the previously computed mean and standard deviation from the training set (X_train).\n",
    "\n",
    "\n",
    "2. This ensures that the test data is scaled using the same parameters as the training data, which is important to avoid data leakage. In machine learning, you should only use information from the training set to scale or transform the test data.\n",
    "\n",
    "\n",
    "3. Why is This Important?\n",
    "\n",
    "The test set should remain \"unseen\" during the training process, including when performing transformations like standardization. By using the transform method, we ensure that the test data is standardized using the same scale as the training data.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Simple Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "'''\n",
    "\n",
    "Purpose:\n",
    "This line imports the LinearRegression model from the sklearn.linear_model module, which is a part of scikit-learn.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression=LinearRegression(n_jobs=-1)\n",
    "# Initializing the LinearRegression object\n",
    "# n_jobs=-1 doing so it will utilise all my computer processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficient or slope:\",regression.coef_)\n",
    "print(\"Intercept:\",regression.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot Training data plot best fit line\n",
    "plt.scatter(X_train,y_train)\n",
    "plt.plot(X_train,regression.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction of test data\n",
    "1. predicted height output= intercept +coef_(Weights)\n",
    "2. y_pred_test =156.470 + 17.29(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction for test data\n",
    "y_pred=regression.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance Metrics\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse=mean_squared_error(y_test,y_pred)\n",
    "mae=mean_absolute_error(y_test,y_pred)\n",
    "rmse=np.sqrt(mse)\n",
    "print(mse)\n",
    "print(mae)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R square \n",
    "Formula\n",
    "\n",
    "**R^2 = 1 - SSR/SST**\n",
    "\n",
    "\n",
    "R^2\t=\tcoefficient of determination\n",
    "SSR\t=\tsum of squares of residuals\n",
    "SST\t=\ttotal sum of squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=r2_score(y_test,y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusted R2 = 1 â€“ [(1-R2)*(n-1)/(n-k-1)]**\n",
    "\n",
    "where:\n",
    "\n",
    "R2: The R2 of the model\n",
    "n: The number of observations\n",
    "k: The number of predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display adjusted R-squared\n",
    "1 - (1-score)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Linear Regression using OLS\n",
    "!pip install statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=sm.OLS(y_train,X_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=model.predict(X_test)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction For new data\n",
    "regression.predict(scaler.transform([[72]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
