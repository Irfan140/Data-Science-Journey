{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "## 1. L1 and L2 loss\n",
    "\n",
    "*L1* and *L2* are two common loss functions in machine learning which are mainly used to minimize the error.\n",
    "\n",
    "**L1 loss function** are also known as **Least Absolute Deviations** in short **LAD**.\n",
    "**L2 loss function** are also known as **Least square errors** in short **LS**.\n",
    "\n",
    "Let's get brief of these two\n",
    "\n",
    "### L1 Loss function\n",
    "It is used to minimize the error which is the sum of all the absolute differences in between the true value and the predicted value.\n",
    "\n",
    "<img src=\".\\Images\\img13.png\">\n",
    "\n",
    "### L2 Loss Function\n",
    "It is also used to minimize the error which is the sum of all the squared differences in between the true value and the pedicted value.\n",
    "\n",
    "<img src=\".\\Images\\img15.png\">\n",
    "\n",
    "**The disadvantage** of the **L2 norm** is that when there are outliers, these points will account for the main component of the loss. For example, the true value is 1, the prediction is 10 times, the prediction value is 1000 once, and the prediction value of the other times is about 1, obviously the loss value is mainly dominated by 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 100 values between -1 and 1\n",
    "x_guess = tf.linspace(-1., 1., 100)\n",
    "\n",
    "# Define the actual value (scalar)\n",
    "x_actual = tf.constant(0., dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_loss = tf.abs((x_guess-x_actual))\n",
    "l2_loss = tf.square((x_guess-x_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tensors to NumPy arrays for plotting\n",
    "x_ = x_guess.numpy()\n",
    "l1_ = l1_loss.numpy()\n",
    "l2_ = l2_loss.numpy()\n",
    "\n",
    "# Plot\n",
    "plt.plot(x_, l1_, label='L1 Loss')\n",
    "plt.plot(x_, l2_, label='L2 Loss')\n",
    "plt.legend()\n",
    "plt.title('L1 vs L2 Loss')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Huber Loss \n",
    "\n",
    "Huber Loss is often used in regression problems. Compared with L2 loss, Huber Loss is less sensitive to outliers(because if the residual is too large, it is a piecewise function, loss is a linear function of the residual).\n",
    "\n",
    "<img src=\".\\Images\\img1.png\">\n",
    "\n",
    "Among them, $\\delta$ is a set parameter, $y$ represents the real value, and $f(x)$ represents the predicted value.\n",
    "\n",
    "The advantage of this is that when the residual is small, the loss function is L2 norm, and when the residual is large, it is a linear function of L1 norm\n",
    "\n",
    "### Pseudo-Huber loss function \n",
    "\n",
    "A smooth approximation of Huber loss to ensure that each order is differentiable.\n",
    "\n",
    "<img src=\".\\Images\\img2.png\">\n",
    "\n",
    "Where $\\delta$ is the set parameter, the larger the value, the steeper the linear part on both sides.\n",
    "\n",
    "<img src=\".\\Images\\img3.png\">\n",
    "\n",
    "## 3.Hinge Loss\n",
    "\n",
    "Hinge loss is often used for binary classification problems, such as ground true: t = 1 or -1, predicted value y = wx + b\n",
    "\n",
    "In the svm classifier, the definition of hinge loss is\n",
    "\n",
    "<img src=\".\\Images\\img4.png\">\n",
    "\n",
    "In other words, the closer the y is to t, the smaller the loss will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_guess2 = tf.linspace(-3.,5.,500)\n",
    "x_actual2 = tf.convert_to_tensor([1.]*500)\n",
    "\n",
    "\n",
    "# Hinge loss calculation\n",
    "hinge_loss = tf.maximum(0., 1. - (x_guess2 * x_actual2))\n",
    "\n",
    "# Convert tensors to NumPy for plotting\n",
    "x_ = x_guess2.numpy()\n",
    "hin_ = hinge_loss.numpy()\n",
    "\n",
    "# Plot\n",
    "plt.plot(x_, hin_, '--', label='Hinge Loss')\n",
    "plt.legend()\n",
    "plt.title('Hinge Loss')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Cross-entropy loss\n",
    "\n",
    "<img src=\".\\Images\\img7.png\">\n",
    "\n",
    "The above is mainly to say that cross-entropy loss is mainly applied to binary classification problems. The predicted value is a probability value and the loss is defined according to the cross entropy. Note the value range of the above value: the predicted value of y should be a probability and the value range is [0,1]\n",
    "\n",
    "<img src=\".\\Images\\img8.png\">\n",
    "\n",
    "## 5.Sigmoid-Cross-entropy loss\n",
    "\n",
    "The above cross-entropy loss requires that the predicted value is a probability. Generally, we calculate $scores = x*w + b$. Entering this value into the sigmoid function can compress the value range to (0,1).\n",
    "\n",
    "<img src=\".\\Images\\img9.png\">\n",
    "\n",
    "It can be seen that the sigmoid function smoothes the predicted value(such as directly inputting 0.1 and 0.01 and inputting 0.1, 0.01 sigmoid and then entering, the latter will obviously have a much smaller change value), which makes the predicted value of sigmoid-ce far from the label loss growth is not so steep.\n",
    "\n",
    "## 6.Softmax cross-entropy loss\n",
    "\n",
    "First, the softmax function can convert a set of fraction vectors into corresponding probability vectors. Here is the definition of softmax function\n",
    "\n",
    "<img src=\".\\Images\\img10.png\">\n",
    "\n",
    "As above, softmax also implements a vector of 'squashes' k-dimensional real value to the [0,1] range of k-dimensional, while ensuring that the cumulative sum is 1.\n",
    "\n",
    "According to the definition of cross entropy, probability is required as input.Sigmoid-cross-entropy-loss uses sigmoid to convert the score vector into a probability vector, and softmax-cross-entropy-loss uses a softmax function to convert the score vector into a probability vector.\n",
    "\n",
    "According to the definition of cross entropy loss.\n",
    "\n",
    "<img src=\".\\Images\\img11.png\">\n",
    "\n",
    "where $p(x)$ represents the probability that classification $x$ is a correct classification, and the value of $p$ can only be 0 or 1. This is the prior value\n",
    "\n",
    "$q(x)$ is the prediction probability that the $x$ category is a correct classification, and the value range is (0,1)\n",
    "\n",
    "So specific to a classification problem with a total of C types, then $p(x_j)$, $(0 <_{=} j <_{=} C)$ must be only 1 and C-1 is 0(because there can be only one correct classification, correct the probability of classification as correct classification is 1, and the probability of the remaining classification as correct classification is 0)\n",
    "\n",
    "Then the definition of softmax-cross-entropy-loss can be derived naturally.\n",
    "\n",
    "Here is the definition of softmax-cross-entropy-loss.\n",
    "\n",
    "<img src=\".\\Images\\img12.png\">\n",
    "\n",
    "Where $f_j$ is the score of all possible categories, and $f_{y_i}$ is the score of ground true class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
